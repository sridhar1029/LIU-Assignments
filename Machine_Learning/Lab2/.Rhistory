max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
setwd("C:/Users/Sridhar/Desktop/ML_assignments_LIU/Lab2")
knitr::opts_chunk$set(echo = TRUE)
set.seed(12345)
#functions
plotTree <- function(tree){
plot(tree,main="Fitted tree")
text(tree, cex=.75)
}
data <- read.table("State.csv",sep=";",header = TRUE)
colsNeeded <- c("EX","MET")
data <- data[colsNeeded]
data$MET <- gsub(',', '.', data$MET)
data$MET <- as.numeric(data$MET)
# reorder
data = data[order(data$MET),]
#plot
plot(EX ~ MET, data = data, pch = 19, cex = 1,col="red", main="EX Vs MET Plot")
#part 2
# fitting tree
model <- tree(formula = EX ~ MET,data = data,control =tree.control(nobs = nrow(data),minsize = 8))
library(fastICA)
library(ggplot2)
library(tree)
library(readxl)
library(e1071)
#part 2
# fitting tree
model <- tree(formula = EX ~ MET,data = data,control =tree.control(nobs = nrow(data),minsize = 8))
cat("\nFitted Tree:")
plotTree(model)
cvModel <- cv.tree(model)
plot(cvModel,main="CV Plot: Deviance vs tree Size")
# Plotting cv tree
plot(cvModel$size, cvModel$dev, main = "Deviance Vs Size" ,
xlab="size", ylab = "deviance", type="b",col="blue", pch= 19,cex=1)
# which size is better?
bestSize <- cvModel$size[which(cvModel$dev==min(cvModel$dev))]
cat("\n Optimal tree:",bestSize)
bestTree <- prune.tree(model,best = bestSize)
# predictions
preds = predict(bestTree,newdata=data)
# plot original and fitted data
results  <- data.frame(Ind=data$MET,original=data$EX,predicted=preds)
ggplot(results, aes(Ind, y = value, color = variable)) +
geom_point(aes(y = original, col = "original")) +
geom_point(aes(y = predicted, col = "predicted"))+
ggtitle("Predicted Vs original using optimal tree of size 3")
#hsitogram
resids<- (data$EX - preds)
hist(resids)
#part 3
library(boot)
set.seed(12345)
bootstrap <- function(data,i){
data  <- data[i,]
model <- tree(EX ~ MET, data = data, control = tree.control(nobs = nrow(data),minsize = 8))
bestTree <- prune.tree(model,best = bestSize)
preds <- predict(bestTree,newdata=data)
return(preds)
}
bootResults <- boot(data=data,statistic = bootstrap,R=1000)
#resConf = boot.ci(bootResults, type="bca")
confBound <- envelope(bootResults,level = 0.95)
upperLimits <- confBound$point[1,]
lowerLimits <- confBound$point[2,]
results["upper"] = upperLimits
results["lower"] = lowerLimits
ggplot(results, aes(Ind,original,predicted,upper,lower))+
geom_point(aes(Ind,original),color="red")+
geom_point(aes(Ind,predicted),color="blue")+
geom_line(aes(Ind,upper),color="green")+
geom_line(aes(Ind,lower),color="green")+
ggtitle("Confidence bound")
# part4, parammetric bootstraping
bootStrapParam <- function(data,index){
data <- data[index,]
model <- tree(EX ~ MET, data = data, control = tree.control(nobs = nrow(data),minsize = 8))
bestTree <- prune.tree(model,best = bestSize)
preds <- predict(bestTree,newdata=data)
resids <- data$EX - preds
# each prediction is an estimation and can be used as mean,
stDev <- sd(resids)
preds<- rnorm(nrow(data),preds,stDev)
return(preds)
}
ranGenFunc <- function(data,model){
data$EX = rnorm(nrow(data), predict(model,newdata=data),sd(resid(model)))
return(data)
}
bootResults <- boot(data,statistic = bootStrapParam , R=1000, mle=bestTree,sim="parametric",ran.gen = ranGenFunc)
confBound <- envelope(bootResults,level = 0.95)
upperLimits <- confBound$point[1,]
lowerLimits <- confBound$point[2,]
results["upperP"] = upperLimits
results["lowerP"] = lowerLimits
ggplot(results, aes(Ind,original,predicted,upperP,lowerP))+
geom_point(aes(Ind,original),color="red")+
geom_point(aes(Ind,predicted),color="blue")+
geom_line(aes(Ind,upperP),color="green")+
geom_line(aes(Ind,lowerP),color="green")+
ggtitle("Prediction band")
hist(resids)
library(boot)
#data2=data[order(data$Area),]#reorderingdata accordingtoArea
#computingbootstrapsamples
f=function(data, ind){
data1=data[ind,]# extractbootstrapsample
res=lm(Price~Area, data=data1) #fit linearmodel
#predictvaluesfor all Area valuesfrom the original data
priceP=predict(res,newdata=data2)
return(priceP)
}
res=boot(data, f, R=1000) #make bootstrap
library(boot)
#data2=data[order(data$Area),]#reorderingdata accordingtoArea
#computingbootstrapsamples
f=function(data, ind){
data1=data[ind,]# extractbootstrapsample
res=lm(EX~MET, data=data) #fit linearmodel
#predictvaluesfor all Area valuesfrom the original data
priceP=predict(res,newdata=data2)
return(priceP)
}
res=boot(data, f, R=1000) #make bootstrap
library(boot)
#data2=data[order(data$Area),]#reorderingdata accordingtoArea
#computingbootstrapsamples
f=function(data, ind){
data1=data[ind,]# extractbootstrapsample
res=lm(EX~MET, data=data) #fit linearmodel
#predictvaluesfor all Area valuesfrom the original data
priceP=predict(res,newdata=data)
return(priceP)
}
res=boot(data, f, R=1000) #make bootstrap
#part 3
# library(boot)
# set.seed(12345)
# bootstrap <- function(data,i){
#   data  <- data[i,]
#
#   model <- tree(EX ~ MET, data = data, control = tree.control(nobs = nrow(data),minsize = 8))
#   bestTree <- prune.tree(model,best = bestSize)
#   preds <- predict(bestTree,newdata=data)
#   return(preds)
# }
# bootResults <- boot(data=data,statistic = bootstrap,R=1000)
#
# #resConf = boot.ci(bootResults, type="bca")
#
# confBound <- envelope(bootResults,level = 0.95)
#
# upperLimits <- confBound$point[1,]
# lowerLimits <- confBound$point[2,]
#
# results["upper"] = upperLimits
# results["lower"] = lowerLimits
#
# ggplot(results, aes(Ind,original,predicted,upper,lower))+
#   geom_point(aes(Ind,original),color="red")+
#   geom_point(aes(Ind,predicted),color="blue")+
#   geom_line(aes(Ind,upper),color="green")+
#   geom_line(aes(Ind,lower),color="green")+
#   ggtitle("Confidence bound")
library(boot)
#data2=data[order(data$Area),]#reorderingdata accordingtoArea
#computingbootstrapsamples
f=function(data, ind){
data1=data[ind,]# extractbootstrapsample
res=lm(EX~MET, data=data) #fit linearmodel
#predictvaluesfor all Area valuesfrom the original data
priceP=predict(res,newdata=data)
return(priceP)
}
res=boot(data, f, R=1000) #make bootstrap
plot(res)
#part 3
# library(boot)
# set.seed(12345)
# bootstrap <- function(data,i){
#   data  <- data[i,]
#
#   model <- tree(EX ~ MET, data = data, control = tree.control(nobs = nrow(data),minsize = 8))
#   bestTree <- prune.tree(model,best = bestSize)
#   preds <- predict(bestTree,newdata=data)
#   return(preds)
# }
# bootResults <- boot(data=data,statistic = bootstrap,R=1000)
#
# #resConf = boot.ci(bootResults, type="bca")
#
# confBound <- envelope(bootResults,level = 0.95)
#
# upperLimits <- confBound$point[1,]
# lowerLimits <- confBound$point[2,]
#
# results["upper"] = upperLimits
# results["lower"] = lowerLimits
#
# ggplot(results, aes(Ind,original,predicted,upper,lower))+
#   geom_point(aes(Ind,original),color="red")+
#   geom_point(aes(Ind,predicted),color="blue")+
#   geom_line(aes(Ind,upper),color="green")+
#   geom_line(aes(Ind,lower),color="green")+
#   ggtitle("Confidence bound")
#part 3
library(boot)
set.seed(12345)
bootstrap <- function(data,i){
data  <- data[i,]
model <- tree(EX ~ MET, data = data, control = tree.control(nobs = nrow(data),minsize = 8))
bestTree <- prune.tree(model,best = bestSize)
preds <- predict(bestTree,newdata=data)
return(preds)
}
bootResults <- boot(data=data,statistic = bootstrap,R=1000)
confBound <- envelope(bootResults,level = 0.95)
upperLimits <- confBound$point[1,]
lowerLimits <- confBound$point[2,]
results["upper"] = upperLimits
results["lower"] = lowerLimits
ggplot(results, aes(Ind,original,predicted,upper,lower))+
geom_point(aes(Ind,original),color="red")+
geom_point(aes(Ind,predicted),color="blue")+
geom_line(aes(Ind,upper),color="green")+
geom_line(aes(Ind,lower),color="green")+
ggtitle("Confidence bound")
##tree deviance
print("Deviance Impurity Measure - ")
print("Performance on Train -")
dtm2_dev = tree(good_bad~., train, split = "deviance")
##1
cds = read_xls("creditscoring.xls")
#colSums(is.na(cds))
cds$purpose[which(is.na(cds$purpose))] = 0
cds$good_bad = as.factor(cds$good_bad)
#split data into train test
n=dim(cds)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=cds[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=cds[id2,]
id3=setdiff(id1,id2)
test=cds[id3,]
##tree deviance
print("Deviance Impurity Measure - ")
print("Performance on Train -")
dtm2_dev = tree(good_bad~., train, split = "deviance")
p1 = predict(dtm2_dev, train, type = "class")
cat("Misclassification Rate : ", sum(p1==train$good_bad)/(nrow(train)), "\n")
table(train$good_bad, p1)
print("Performance on Test -")
p2 = predict(dtm2_dev, test, type = "class")
cat("Misclassification Rate : ", sum(p2==test$good_bad)/(nrow(test)), "\n")
table(test$good_bad, p2)
##tree gini
print("Gini Impurity Measure - ")
print("Performance on Train -")
dtm2_gin = tree(good_bad~., train, split = "gini")
p1 = predict(dtm2_gin, train, type = "class")
cat("Misclassification Rate : ", sum(p1==train$good_bad)/(nrow(train)), "\n")
table(train$good_bad, p1)
print("Performance on test -")
p2 = predict(dtm2_gin, valid, type = "class")
cat("Misclassification Rate : ", sum(p2==test$good_bad)/(nrow(test)), "\n")
table(test$good_bad, p2)
##depth calc
pt_train <- prune.tree(dtm2_dev)
pt_valid <- prune.tree(dtm2_dev,newdata = valid)
{plot(pt_train$size, pt_train$dev, type="b", col="green",ylim=c(260,630),
xlab="Leaf Nodes",ylab="Deviances")
points(x=pt_valid$size,y=pt_valid$dev, type="b", col="red")
title(main="Training vs Validation deviances on pruned trees")
legend("topright",legend=c("Train Data Set", "Validation Data Set"),
col=c("green","red"),lty=1:2, cex=0.8,title="Deviances")}
#Selecting the number of leaf node from the valid set for which the deviance error is least.
best_lf <-  pt_valid$size[which.min(pt_valid$dev)]
#Optinmal Tree based on leat deviance
pt_opt  <- prune.tree(dtm2_dev,best=best_lf)
nodes_train_opt <- as.numeric(rownames(pt_opt$frame))  #getnodes
depth_train <-  max(tree:::tree.depth(nodes_train_opt)) #get depth
{plot(pt_opt)
text(pt_opt)
title(main="Optimal Tree for Training Data")}
cat("Depth of Optimal Tree for training data:",depth_train,"\n")
cat("Number of Leaf Nodes in Optimal Tree for training data:",best_lf,"\n")
cat("\nLabels used by training data:\n")
tree:::labels.tree(pt_opt)
model = naiveBayes(good_bad~., train)
#train
print("Naive Bayes Model - ")
print("Performance on Train -")
pred = predict(model, train[,-ncol(train)])
cat("Misclassification Rate : ", sum(pred==train$good_bad)/(nrow(train)), "\n")
table(train$good_bad, pred)
#test
print("Performance on Test -")
pred = predict(model, test[,-ncol(test)])
cat("Misclassification Rate : ", sum(pred==test$good_bad)/(nrow(test)), "\n")
table(test$good_bad, pred)
#Naive Bayes
pies = seq(0.05, 0.95, 0.05)
results_nb = matrix(nrow = 0, ncol = 3)
results_ot = matrix(nrow = 0, ncol = 3)
for(pi in pies){
pred = as.data.frame(predict(model, test[, -ncol(test)], type = "raw"))
pred$res = ifelse(pred$good>pi, "good", "bad")
misCl = sum(pred$res == test$good_bad)/(nrow(pred))
m = (test$good_bad == "good")*1
n = (pred$res == "good")*1
tp = sum(m*n)
fp = abs(sum(n)-tp)/sum(abs(m-1))
tp = tp/(sum(m))
results_nb = rbind(results_nb, c(misCl, tp, fp))
pred = as.data.frame(predict(pt_opt, test[, -ncol(test)]))
pred$res = ifelse(pred$good>pi, "good", "bad")
misCl = sum(pred$res == test$good_bad)/(nrow(pred))
m = (test$good_bad == "good")*1
n = (pred$res == "good")*1
tp = sum(m*n)
fp = (abs(sum(n)-tp))/sum(abs(m-1))
tp = tp/(sum(m))
results_ot = rbind(results_ot, c(misCl, tp, fp))
}
results_nb = as.data.frame(results_nb)
colnames(results_nb) = c("MiscRate", "TP", "FP")
results_ot = as.data.frame(results_ot)
colnames(results_ot) = c("MiscRate", "TP", "FP")
ggplot() + geom_line(data=results_nb,aes(x=FP,y=TP,color="red")) +
geom_line(data=results_ot,aes(x=FP,y=TP,color="blue"))+ scale_color_discrete(name="Model",labels=c("Naive Bayes","Optimal Tree")) +
geom_abline(intercept=0,slope=1)+
xlab("FPR")+ylab("TPR")+ggtitle("ROC curve between Naive Bayes and Optimal Tree")
model = naiveBayes(good_bad~., train)
#train
print("Naive Bayes Model - ")
print("Performance on Train -")
pred = as.data.frame(predict(model, train[,-ncol(train)], type = "raw"))
pred$res = ifelse((pred$good)>(pred$bad)*10, "good", "bad")
cat("Misclassification Rate : ", sum(pred$res==train$good_bad)/(nrow(train)), "\n")
table(train$good_bad, pred$res)
#test
print("Performance on Test -")
pred = as.data.frame(predict(model, test[,-ncol(test)], type = "raw"))
pred$res = ifelse((pred$good)>(pred$bad)*10, "good", "bad")
cat("Misclassification Rate : ", sum(pred$res==test$good_bad)/(nrow(test)), "\n")
table(test$good_bad, pred$res)
