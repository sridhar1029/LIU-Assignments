true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=10 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
i=1
j=1
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
View(z)
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
i=1
j=1
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
View(z)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
View(z)
# #Log likelihood computation.
# # Your code here
for (i in 1:length(pi)) {
#i = 1
for (j in 1:ncol(x)) {
#j = 1
llik[it] <- sum(pi[i] * dbinom(x[,j], K, mu[ ,j]))
}
}
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
# #Log likelihood computation.
# # Your code here
for (i in 1:length(pi)) {
#i = 1
for (j in 1:ncol(x)) {
#j = 1
llik[it] <- sum(pi[i] * dbinom(x[,j], K, mu[ ,j]))
}
}
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
# if(abs(old-llik[it])<min_change){
#   break
# }
# else{
#   old = llik[it]
# }
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = sum(total)
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
setwd("C:/Users/Sridhar/Desktop/ML_assignments_LIU/Lab1Block2")
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
library(mboost)
library(randomForest)
library(ggplot2)
spam_data <- read.csv2("spambase.csv")
spam_data$Spam <- as.factor(spam_data$Spam)
n=dim(spam_data)[1]
set.seed(12345)
ids=sample(1:n,floor(0.75*n))
spam_train=spam_data[ids,]
spam_test=spam_data[-ids,]
misclas_test <- c()
comp<-matrix(0,nrow=0,ncol=2)
colnames(comp) <- c("Classes","Error_ada")
for(i in seq(10,100,10)) {
model_boost <-
mboost::blackboost(
Spam ~ .,
data = spam_train,
family = AdaExp(),
control = boost_control(mstop = i)
)
predict_test<-predict(model_boost,newdata=spam_test,type="class")
comp_test<-data.frame("Expected_Value"=spam_test$Spam,
"Predicted_Value"=predict_test)
pred<-table(comp_test)
missclas_test<-(pred[2,1]+pred[1,2])/nrow(spam_test)
comp <- rbind(comp,c(i,missclas_test))
}
comp<-as.data.frame(comp)
ggplot(comp,aes(Classes,Error_ada)) +geom_point() +
geom_line(col='blue') + xlab("Number of Classifiers") +
ylab("Errors")+ggtitle("Num of Modifiers vs Error" )
comp_ran<-matrix(0,nrow=0,ncol=2)
colnames(comp_ran) <- c("Classes","Error_rf")
for(i in seq(10,100,10)) {
set.seed(1234)
model_rforest<-randomForest(Spam~.,data=spam_train,ntree=i)
predict_test<-predict(model_rforest,newdata=spam_test,type="class")
comp_test<-data.frame("Expected_Value"=spam_test$Spam,
"Predicted_Value"=predict_test)
pred<-table(comp_test)
missclas_test<-(pred[2,1]+pred[1,2])/nrow(spam_test)
comp_ran <- rbind(comp_ran,c(i,missclas_test))
}
comp_ran<-as.data.frame(comp_ran)
ggplot(comp_ran,aes(Classes,Error_rf)) +geom_point() + geom_line(col="blue")+
coord_cartesian(ylim=c(0.04,0.1)) + xlab("Number of Sample Trees") +
ylab("Errors")+ggtitle("Num of Sample Trees vs Error" )
