plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
i=1
j=1
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
View(z)
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
i=1
j=1
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
View(z)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
View(z)
# #Log likelihood computation.
# # Your code here
for (i in 1:length(pi)) {
#i = 1
for (j in 1:ncol(x)) {
#j = 1
llik[it] <- sum(pi[i] * dbinom(x[,j], K, mu[ ,j]))
}
}
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
# #Log likelihood computation.
# # Your code here
for (i in 1:length(pi)) {
#i = 1
for (j in 1:ncol(x)) {
#j = 1
llik[it] <- sum(pi[i] * dbinom(x[,j], K, mu[ ,j]))
}
}
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
# if(abs(old-llik[it])<min_change){
#   break
# }
# else{
#   old = llik[it]
# }
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = sum(total)
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.01 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 2) # true mixing coefficients
true_mu <- matrix(nrow=2, ncol=D) # true conditional distributions
true_pi=c(1/2,1/2)#,1/3)
true_mu[1,]=c(0.5, 0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.7, 0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
#true_mu[3,]=c(0.3, 0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
#points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:2,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=2 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
old = 0
for(it in 1:max_it) {
plot(mu[1,], type="o", col="blue", ylim=c(0,1))
points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments
# Your code here
Estep <- list()
# denominator
total <- matrix(0, nrow = nrow(x), ncol= 1)
z = matrix(1, nrow = nrow(x), ncol = length(pi))
for (i in 1:length(pi)) {
for (j in 1:ncol(x)) {
z[, i] = z[, i] * dbinom(x[ ,j], 1, mu[i,j])
}
z[, i] = z[, i] * pi[i]
total= total + z[,i]
}
for(i in 1:length(pi)){
z[,i] = z[,i]/total
}
#Log likelihood computation.
# # Your code here
llik[it] = log(sum(total))
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the log likelihood has not changed significantly
# Your code here
if(abs(old-llik[it])<min_change){
break
}
else{
old = llik[it]
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
new_mu = matrix(nrow = nrow(mu), ncol = ncol(mu))
new_pi = colSums(z)/nrow(x)
for (i in 1:length(pi)){
nm = sum(z[,i])
new_mu[i,] = colSums(x*z[,i])/nm
}
mu = new_mu
pi = new_pi
}
setwd("C:/Users/Sridhar/Desktop/ML_assignments_LIU/Lab3")
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
data("spam",package="kernlab")
spamdata <- spam
nr <- nrow(spamdata)
set.seed(12345)
id <- sample(1:nr,floor(0.8*nr))
spamdata_train <- spamdata[id,]
id2 <- setdiff(1:nr,id)
id3 <- sample(id2,floor(0.5*length(id2)))
id4 <- setdiff(id2,id3)
spamdata_valid <- spamdata[id3,]
spamdata_test <- spamdata[id4,]
#rbfdot is the radial basis function for the kernel type.
#C signifies the cost for residuals. Higher cost would result in
#high variance and less bias.
#With C=0.5
svm_model <- ksvm(type~., data=spamdata_train,kernel="rbfdot",kpar=list(sigma=0.05),
prob.model=TRUE,C=0.5)
library(kernlab)
library(tidyverse)
library(geosphere)
#Assignment 1
kernal_methods <- function(st, date, lat, lon, time_seq, h_date, h_time, h_distance){
st$h_dist = abs(distHaversine(p1 = c(lon, lat), p2 = st[,c("longitude", "latitude")]))
st$h_dist = exp(-(st$h_dist/h_distance)^2)
st$h_date = as.numeric(difftime(date, st$date, units = c("days")))
st$h_date = ifelse(st$h_date>0, st$h_date, 0)
st = subset(st, st$h_date!=0)
st$h_date = exp(-(st$h_date/h_date)^2)
times = c()
for(t in 1:length(time_seq)){
d = as.Date(time_seq[t])
c_time = format(time_seq[t], "%H:%M:%S")
times = append(times, c(c_time))
st[c_time] =  as.numeric(abs(difftime(strptime(paste(d, c_time),
"%Y-%m-%d%H:%M:%S"),
strptime(paste(d, st$time),
"%Y-%m-%d%H:%M:%S"),
units = c("hour"))))
st[c_time] = exp(-(st[c_time]/h_time)^2)
}
temp = st[times]
temp_add_d = temp + (st$h_date + st$h_dist)
temp_mul_d = temp * (st$h_date * st$h_dist)
temp_add_n = temp_add_d*st$air_temperature
temp_mul_n = temp_mul_d*(st$air_temperature)
temp_add = colSums(temp_add_n)/colSums(temp_add_d)
temp_mul = colSums(temp_mul_n)/colSums(temp_mul_d)
d = data.frame(Index = 1:length(times), Time = times, Add = temp_add, Mul = temp_mul)
return(d)
}
set.seed(1234567890)
stations = read.csv("stations.csv")
temps = read.csv("temps50k.csv")
st <- merge(stations,temps,by="station_number")
#colnames(st)
h_distance <- 30000
h_date <- 12
h_time <- 4
lat <- 58.4274 # The point to predict (up to the students)
lon <- 14.826
date <- "2014-05-04" # The date to predict (up to the students)
start <- as.POSIXct(date)
interval <- 60
end <- start + as.difftime(1, units="days")
time_seq <- seq(from=start, by=interval*120, to=end)
time_seq = time_seq[3:length(time_seq)]
pred = kernal_methods(st, date, lat, lon, time_seq, h_date, h_time, h_distance)
ggplot(pred) + geom_line(aes(Index, Add, col="Add")) + geom_line(aes(Index, Mul, col="Mul"))
data("spam",package="kernlab")
spamdata <- spam
nr <- nrow(spamdata)
set.seed(12345)
id <- sample(1:nr,floor(0.8*nr))
spamdata_train <- spamdata[id,]
id2 <- setdiff(1:nr,id)
id3 <- sample(id2,floor(0.5*length(id2)))
id4 <- setdiff(id2,id3)
spamdata_valid <- spamdata[id3,]
spamdata_test <- spamdata[id4,]
#rbfdot is the radial basis function for the kernel type.
#C signifies the cost for residuals. Higher cost would result in
#high variance and less bias.
#With C=0.5
svm_model <- ksvm(type~., data=spamdata_train,kernel="rbfdot",kpar=list(sigma=0.05),
prob.model=TRUE,C=0.5)
predict1 <- predict(svm_model,newdata=spamdata_valid,type="response")
#With C=1
svm_model2 <- ksvm(type~., data=spamdata_train,kernel="rbfdot",kpar=list(sigma=0.05),
prob.model=TRUE,C=1)
predict2 <- predict(svm_model2,newdata=spamdata_valid,type="response")
#With C=5
svm_model3 <- ksvm(type~., data=spamdata_train,kernel="rbfdot",kpar=list(sigma=0.05),
prob.model=TRUE,C=5)
predict3 <- predict(svm_model3,newdata=spamdata_valid,type="response")
conf_table <- matrix(nrow=0,ncol=3)
colnames(conf_table) <- c("C=0.5","C=1","C=5")
conf_matrix1 <- table("Expected" = spamdata_valid$type,"Predicted"=predict1)
missclas1 <-  sum(conf_matrix1[1,2]+conf_matrix1[2,1])/nrow(spamdata_valid)
conf_matrix2 <- table("Expected" = spamdata_valid$type,"Predicted"=predict2)
missclas2 <-  sum(conf_matrix2[1,2]+conf_matrix2[2,1])/nrow(spamdata_valid)
conf_matrix3 <- table("Expected" = spamdata_valid$type,"Predicted"=predict3)
missclas3 <-  sum(conf_matrix3[1,2]+conf_matrix3[2,1])/nrow(spamdata_valid)
conf_table <- rbind(conf_table,c(missclas1,missclas2,missclas3))
rownames(conf_table) <- "Missclass_Error"
knitr::kable(conf_table,caption="Missclassification Error on Validation Set")
spamdata_train2 <-spamdata[c(id,id3),]
svm_model <- ksvm(type~.,data=spamdata_train2,kernel="rbfdot",kpar=list(sigma=0.05)
,C=1)
predict <- predict(svm_model,newdata=spamdata_test)
conf_matrix <- table("Expected" = spamdata_test$type,"Predicted"=predict)
cat("Confusion Matrix for C=1 :\n")
conf_matrix
missclas <-  sum(conf_matrix[1,2]+conf_matrix[2,1])/nrow(spamdata_test)
cat("\n\nMissclassification Error with C=1:",missclas)
